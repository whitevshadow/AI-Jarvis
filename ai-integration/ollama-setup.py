#!/usr/bin/env python3
"""
Ollama Setup and Integration Script for JARVIS
This script helps set up Ollama with DeepSeek-R1 model for enhanced AI capabilities
"""

import subprocess
import sys
import requests
import json
import time
import os
from pathlib import Path

class OllamaSetup:
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.model_name = "deepseek-r1"
        
    def check_ollama_installed(self):
        """Check if Ollama is installed"""
        try:
            result = subprocess.run(['ollama', '--version'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print("âœ… Ollama is installed")
                return True
            else:
                print("âŒ Ollama is not installed")
                return False
        except FileNotFoundError:
            print("âŒ Ollama is not installed")
            return False
    
    def check_ollama_running(self):
        """Check if Ollama service is running"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                print("âœ… Ollama service is running")
                return True
            else:
                print("âŒ Ollama service is not responding")
                return False
        except requests.exceptions.RequestException:
            print("âŒ Ollama service is not running")
            return False
    
    def start_ollama_service(self):
        """Start Ollama service"""
        print("ðŸ”„ Starting Ollama service...")
        try:
            if os.name == 'nt':  # Windows
                subprocess.Popen(['ollama', 'serve'], 
                               creationflags=subprocess.CREATE_NEW_CONSOLE)
            else:  # Linux/Mac
                subprocess.Popen(['ollama', 'serve'])
            
            # Wait for service to start
            for i in range(10):
                time.sleep(2)
                if self.check_ollama_running():
                    return True
                print(f"â³ Waiting for Ollama to start... ({i+1}/10)")
            
            print("âŒ Failed to start Ollama service")
            return False
        except Exception as e:
            print(f"âŒ Error starting Ollama: {e}")
            return False
    
    def check_model_available(self):
        """Check if DeepSeek-R1 model is available"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get('models', [])
                for model in models:
                    if 'deepseek' in model['name'].lower():
                        print(f"âœ… DeepSeek model found: {model['name']}")
                        return True
                print("âŒ DeepSeek model not found")
                return False
            else:
                print("âŒ Failed to check available models")
                return False
        except Exception as e:
            print(f"âŒ Error checking models: {e}")
            return False
    
    def pull_model(self):
        """Pull DeepSeek-R1 model"""
        print(f"ðŸ“¥ Pulling {self.model_name} model...")
        print("âš ï¸  This may take several minutes depending on your internet connection")
        
        try:
            # Use subprocess for better control
            process = subprocess.Popen(
                ['ollama', 'pull', self.model_name],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            # Monitor the process
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    print(f"ðŸ“¥ {output.strip()}")
            
            if process.returncode == 0:
                print(f"âœ… {self.model_name} model pulled successfully")
                return True
            else:
                error = process.stderr.read()
                print(f"âŒ Failed to pull model: {error}")
                return False
                
        except Exception as e:
            print(f"âŒ Error pulling model: {e}")
            return False
    
    def test_model(self):
        """Test the model with a simple query"""
        print("ðŸ§ª Testing model...")
        
        try:
            payload = {
                "model": self.model_name,
                "prompt": "Hello, I am JARVIS. Please respond as an AI assistant.",
                "stream": False
            }
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                print("âœ… Model test successful")
                print(f"ðŸ¤– Response: {result.get('response', 'No response')[:100]}...")
                return True
            else:
                print(f"âŒ Model test failed: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ Error testing model: {e}")
            return False
    
    def setup(self):
        """Complete setup process"""
        print("ðŸš€ Setting up Ollama with DeepSeek-R1 for JARVIS")
        print("=" * 50)
        
        # Check if Ollama is installed
        if not self.check_ollama_installed():
            print("\nðŸ“‹ Please install Ollama first:")
            print("ðŸŒ Visit: https://ollama.ai/")
            print("ðŸ’» Or run: curl -fsSL https://ollama.ai/install.sh | sh")
            return False
        
        # Check if service is running
        if not self.check_ollama_running():
            if not self.start_ollama_service():
                print("\nðŸ“‹ Please start Ollama manually:")
                print("ðŸ’» Run: ollama serve")
                return False
        
        # Check if model is available
        if not self.check_model_available():
            if not self.pull_model():
                return False
        
        # Test the model
        if not self.test_model():
            return False
        
        print("\nðŸŽ‰ Setup completed successfully!")
        print("ðŸ¤– DeepSeek-R1 is ready for JARVIS")
        print("\nðŸ“‹ Next steps:")
        print("1. Start the JARVIS web server: npm run dev")
        print("2. Open http://localhost:3000 in your browser")
        print("3. Enjoy enhanced AI capabilities!")
        
        return True

def main():
    setup = OllamaSetup()
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == 'check':
            setup.check_ollama_installed()
            setup.check_ollama_running()
            setup.check_model_available()
        elif command == 'pull':
            setup.pull_model()
        elif command == 'test':
            setup.test_model()
        elif command == 'start':
            setup.start_ollama_service()
        else:
            print("Usage: python ollama-setup.py [check|pull|test|start]")
    else:
        setup.setup()

if __name__ == "__main__":
    main()